{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some modules from other libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "# Import the environment module\n",
    "from environment import Environment\n",
    "from qnetworkviz import QNetworkViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Agent class allows the agent to interact with the environment.\n",
    "class Agent:\n",
    "\n",
    "    # The class initialisation function.\n",
    "    def __init__(self, environment):\n",
    "        # Set the agent's environment.\n",
    "        self.environment = environment\n",
    "        # Create the agent's current state\n",
    "        self.state = None\n",
    "        # Create the agent's total reward for the current episode.\n",
    "        self.total_reward = None\n",
    "        # Reset the agent.\n",
    "        self.reset()\n",
    "\n",
    "    # Function to reset the environment, and set the agent to its initial state. This should be done at the start of every episode.\n",
    "    def reset(self):\n",
    "        # Reset the environment for the start of the new episode, and set the agent's state to the initial state as defined by the environment.\n",
    "        self.state = self.environment.reset()\n",
    "        # Set the agent's total reward for this episode to zero.\n",
    "        self.total_reward = 0.0\n",
    "        \n",
    "\n",
    "    # Function to make the agent take one step in the environment.\n",
    "    def step(self,mode,epsilon=None,action=None):\n",
    "        # Choose an action.\n",
    "        if mode == \"random\":\n",
    "            discrete_action = np.random.randint(4)\n",
    "            \n",
    "        elif mode == \"egreedy\":\n",
    "            actions = np.arange(0,4)\n",
    "            probas = np.ones(4)*(epsilon/4)\n",
    "            probas[action] = 1-epsilon + (epsilon/4)\n",
    "            discrete_action = np.random.choice(actions,p=probas)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"mode must be in [random,egreedy]\")\n",
    "            \n",
    "        # Convert the discrete action into a continuous action.\n",
    "        continuous_action = self._discrete_action_to_continuous(discrete_action)\n",
    "        # Take one step in the environment, using this continuous action, based on the agent's current state. This returns the next state, and the new distance to the goal from this new state. It also draws the environment, if display=True was set when creating the environment object..\n",
    "        next_state, distance_to_goal = self.environment.step(self.state, continuous_action)\n",
    "        # Compute the reward for this paction.\n",
    "        reward = self._compute_reward(distance_to_goal)\n",
    "        # Create a transition tuple for this step.\n",
    "        transition = (self.state, discrete_action, reward, next_state)\n",
    "        # Set the agent's state for the next step, as the next state from this step\n",
    "        self.state = next_state\n",
    "        # Update the agent's reward for this episode\n",
    "        self.total_reward += reward\n",
    "        # Return the transition\n",
    "        return transition\n",
    "\n",
    "    # Function for the agent to compute its reward. In this example, the reward is based on the agent's distance to the goal after the agent takes an action.\n",
    "    def _compute_reward(self, distance_to_goal):\n",
    "        reward = 1 - distance_to_goal\n",
    "        return reward\n",
    "\n",
    "    # Function to convert discrete action (as used by a DQN) to a continuous action (as used by the environment).\n",
    "    def _discrete_action_to_continuous(self, discrete_action):\n",
    "        if discrete_action == 0:  # Move right\n",
    "            continuous_action = np.array([0.1, 0], dtype=np.float32)\n",
    "        elif discrete_action == 1:#Move down\n",
    "            continuous_action = np.array([0, -0.1], dtype=np.float32)\n",
    "        elif discrete_action == 2:#Move left\n",
    "            continuous_action = np.array([-0.1, 0], dtype=np.float32)\n",
    "        else :#Move up\n",
    "            continuous_action = np.array([0, 0.1], dtype=np.float32)\n",
    "        return continuous_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Network class inherits the torch.nn.Module class, which represents a neural network.\n",
    "class Network(torch.nn.Module):\n",
    "\n",
    "    # The class initialisation function. This takes as arguments the dimension of the network's input (i.e. the dimension of the state), and the dimension of the network's output (i.e. the dimension of the action).\n",
    "    def __init__(self, input_dimension, output_dimension):\n",
    "        # Call the initialisation function of the parent class.\n",
    "        super(Network, self).__init__()\n",
    "        # Define the network layers. This example network has two hidden layers, each with 100 units.\n",
    "        self.layer_1 = torch.nn.Linear(in_features=input_dimension, out_features=100)\n",
    "        self.layer_2 = torch.nn.Linear(in_features=100, out_features=100)\n",
    "        self.output_layer = torch.nn.Linear(in_features=100, out_features=output_dimension)\n",
    "\n",
    "    # Function which sends some input data through the network and returns the network's output. In this example, a ReLU activation function is used for both hidden layers, but the output layer has no activation function (it is just a linear layer).\n",
    "    def forward(self, input):\n",
    "        layer_1_output = torch.nn.functional.relu(self.layer_1(input))\n",
    "        layer_2_output = torch.nn.functional.relu(self.layer_2(layer_1_output))\n",
    "        output = self.output_layer(layer_2_output)\n",
    "        return output\n",
    "\n",
    "class ReplayBuffer(deque):\n",
    "    def __init__(self,min_size):\n",
    "        super().__init__([],10**6)\n",
    "        self.min_size = min_size\n",
    "\n",
    "    def is_full_enough(self):\n",
    "        return len(self)>=self.min_size\n",
    "\n",
    "    def get_minibatch(self):\n",
    "        idx = np.random.choice(np.arange(0,len(self)),self.min_size,replace=False)\n",
    "        try:\n",
    "            minibatch = []\n",
    "            for i in range(len(idx)):\n",
    "                minibatch += [self[idx[i]]]\n",
    "        except:\n",
    "            raise ValueError(\"Replay Buffer is not full enough\")\n",
    "\n",
    "        return np.array(minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DQN class determines how to train the above neural network.\n",
    "class DQN:\n",
    "\n",
    "    # The class initialisation function.\n",
    "    def __init__(self):\n",
    "        # Create a Q-network, which predicts the q-value for a particular state.\n",
    "        self.q_network = Network(input_dimension=2, output_dimension=4)\n",
    "        # Define the optimiser which is used when updating the Q-network. The learning rate determines how big each gradient step is during backpropagation.\n",
    "        self.optimiser = torch.optim.Adam(self.q_network.parameters(), lr=0.001)\n",
    "        self.target_network = Network(input_dimension=2, output_dimension=4)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    # Function that is called whenever we want to train the Q-network. Each call to this function takes in a transition tuple containing the data we use to update the Q-network.\n",
    "    def train_q_network(self, transition):\n",
    "        # Set all the gradients stored in the optimiser to zero.\n",
    "        self.optimiser.zero_grad()\n",
    "        # Calculate the loss for this transition.\n",
    "        loss = self._calculate_loss(transition)\n",
    "        # Compute the gradients based on this loss, i.e. the gradients of the loss with respect to the Q-network parameters.\n",
    "        loss.backward()\n",
    "        # Take one gradient step to update the Q-network.\n",
    "        self.optimiser.step()\n",
    "        # Return the loss as a scalar\n",
    "        loss_value = loss.item()\n",
    "        return loss_value\n",
    "    \n",
    "    \n",
    "    def update_target_network(self):\n",
    "        parameters_q = torch.nn.Module.state_dict(self.q_network)\n",
    "        torch.nn.Module.load_state_dict(self.target_network,parameters_q)\n",
    "\n",
    "    # Function to calculate the loss for a particular transition.\n",
    "    def _calculate_loss(self,transition):\n",
    "\n",
    "        gamma = 0.9\n",
    "        batch_size = len(transition)\n",
    "        transition = np.array(transition).reshape(batch_size,4)\n",
    "        \n",
    "        states_batch = np.array([state.reshape(1,2) for state in transition[:,0]])\n",
    "        next_states_batch = np.array([state.reshape(1,2) for state in transition[:,3]])\n",
    "\n",
    "        ### extracting batches array from transition\n",
    "        \n",
    "        input_batch =  states_batch.reshape(batch_size,2).astype(np.float32)\n",
    "        action_batch = transition[:,1].reshape(batch_size,1).astype(np.long)\n",
    "        reward_batch = transition[:,2].reshape(batch_size,1).astype(np.float32)\n",
    "        next_states_batch = next_states_batch.reshape(batch_size,2).astype(np.float32)\n",
    "                \n",
    "        ### transforming arrays into tensors\n",
    "        \n",
    "        input_tensor = torch.tensor(input_batch)\n",
    "        action_tensor = torch.tensor(action_batch)\n",
    "        reward_tensor = torch.tensor(reward_batch)\n",
    "        next_states_tensor = torch.tensor(next_states_batch)\n",
    "\n",
    "        ### Computing the predicted Q value for state s\n",
    "        \n",
    "        q_values = self.q_network.forward(input_tensor)\n",
    "        q_values = torch.gather(q_values,1,action_tensor)\n",
    "\n",
    "        ### Computing the actual Q value for this step\n",
    "        \n",
    "        #target_q_values = self.q_network.forward(next_states_tensor)\n",
    "        target_q_values = self.target_network.forward(next_states_tensor)\n",
    "        target_q_values = torch.gather(target_q_values,1, target_q_values.argmax(1).view(batch_size,1))\n",
    "        \n",
    "        predicted_sum_future_rewards = reward_tensor.add(target_q_values*gamma)\n",
    "        loss = torch.nn.MSELoss()(q_values, predicted_sum_future_rewards)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def get_greedy_action(self,state,mode):\n",
    "        best_reward = -100\n",
    "        best_action = -100\n",
    "        \n",
    "        if mode==0:\n",
    "            for i in range(4):\n",
    "                input_state_action = np.append(state,i).reshape(1,3).astype(np.float32)\n",
    "                input_tensor = torch.tensor(input_state_action)\n",
    "                predicted_reward = self.q_network.forward(input_tensor)\n",
    "            \n",
    "                if predicted_reward>best_reward:\n",
    "                    best_reward = predicted_reward\n",
    "                    best_action = i\n",
    "                    \n",
    "                    \n",
    "        elif mode==1:\n",
    "            input_tensor = torch.tensor(state.reshape(1,2).astype(np.float32))\n",
    "            q_values = self.q_network.forward(input_tensor)\n",
    "            best_action = int(q_values.argmax(1).numpy())\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('mode must be either 0 or 1') \n",
    "                \n",
    "        return best_action\n",
    "    \n",
    "    def get_qvalues(self,mode):\n",
    "        \n",
    "        if mode == 0:\n",
    "            qvalues = np.zeros((10,10,4))\n",
    "            for i in range(10):\n",
    "                for j in range(10):\n",
    "                    for action in range(4):\n",
    "                        input_state_action = np.array([(0.05 + i*0.1),(0.05 + j*0.1),action]).reshape(1,3).astype(np.float32)\n",
    "                        input_tensor = torch.tensor(input_state_action)\n",
    "                        predicted_qvalue = self.q_network.forward(input_tensor)\n",
    "                        qvalues[i,j,action] = predicted_qvalue\n",
    "            return qvalues\n",
    "        \n",
    "        elif mode ==1 :\n",
    "            qvalues = np.zeros((10,10,4))\n",
    "            for i in range(10):\n",
    "                for j in range(10):\n",
    "                    input_state = np.array([(0.05 + i*0.1),(0.05 + j*0.1)]).reshape(1,2).astype(np.float32)\n",
    "                    input_tensor = torch.tensor(input_state)\n",
    "                    predicted_qvalues = self.q_network.forward(input_tensor)\n",
    "                    qvalues[i,j,:] = predicted_qvalues.detach().numpy().reshape(4)\n",
    "            return qvalues\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('mode must be either 0 or 1')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CID = 799240\n",
    "np.random.seed(CID)\n",
    "torch.manual_seed(CID)\n",
    "\n",
    "environment = Environment(display=False, magnification=500)\n",
    "agent = Agent(environment)\n",
    "dqn = DQN()\n",
    "replay_buffer = ReplayBuffer(min_size=50)\n",
    "n_steps = 0\n",
    "loss_vect = []\n",
    "\n",
    "while n_steps<500:\n",
    "    # Reset the environment for the start of the episode.\n",
    "    agent.reset()\n",
    "    # Loop over steps within this episode. The episode length here is 20.\n",
    "    for step_num in range(20):\n",
    "        n_steps+=1\n",
    "        # Step the agent once, and get the transition tuple for this step\n",
    "        if not replay_buffer.is_full_enough():\n",
    "            transition = agent.step('random')\n",
    "            replay_buffer.append(transition)\n",
    "        else:\n",
    "            #best_action = dqn.get_greedy_action(agent.state,mode=1)\n",
    "            transition = agent.step('random')\n",
    "            #transition = agent.step('egreedy',epsilon = 1/(1+(n_steps%20)), action=best_action)\n",
    "            replay_buffer.append(transition)\n",
    "            mini_batch = replay_buffer.get_minibatch()\n",
    "            loss = dqn.train_q_network(mini_batch)\n",
    "            loss_vect+=[loss]\n",
    "            \n",
    "    dqn.update_target_network()\n",
    "            \n",
    "plt.plot(np.arange(50,n_steps),loss_vect)\n",
    "plt.xticks(np.arange(50,501,50))\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Value of the loss function at each step of training with target network')\n",
    "plt.savefig('figs/fig3b.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test greedy policy\n",
    "\n",
    "test_env = environment = Environment(display=True, magnification=500)\n",
    "agent = Agent(test_env)\n",
    "agent.reset()\n",
    "greedy_path = np.array([agent.state])\n",
    "time.sleep(1)\n",
    "for step_num in range(20):\n",
    "    best_action = dqn.get_greedy_action(agent.state,1)\n",
    "    transition = agent.step('egreedy',epsilon = 0, action=best_action)\n",
    "    greedy_path = np.append(greedy_path,[agent.state],axis=0)\n",
    "    \n",
    "print(greedy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = QNetworkViz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_q_values(dqn.get_qvalues(mode=1))\n",
    "viz.plot_grid()\n",
    "viz.plot_path(greedy_path)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
