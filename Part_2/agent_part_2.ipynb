{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some modules from other libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "# Import the environment module\n",
    "from random_environment import Environment\n",
    "#from qnetworkviz import QNetworkViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    # Function to initialise the agent\n",
    "    def __init__(self):\n",
    "        # Set the episode length\n",
    "        self.episode_length = 1000\n",
    "        # Reset the total number of steps which the agent has taken\n",
    "        self.num_steps_taken = 0\n",
    "        # The state variable stores the latest state of the agent in the environment\n",
    "        self.state = None\n",
    "        # The action variable stores the latest action which the agent has applied to the environment\n",
    "        self.action = None\n",
    "        # DeepQNetwork defining agent actions\n",
    "        self.dqn = DQN()\n",
    "        # Replay Buffer\n",
    "        self.replay_buffer = ReplayBuffer(min_size=50)\n",
    "        #Reward\n",
    "        self.total_reward = 0\n",
    "        #epsilon\n",
    "        self.epsilon = 1\n",
    "\n",
    "\n",
    "    # Function to check whether the agent has reached the end of an episode\n",
    "    def has_finished_episode(self):\n",
    "        if self.num_steps_taken % self.episode_length == 0:\n",
    "            self.epsilon = 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Function to get the next action, using whatever method you like\n",
    "    def get_next_action(self, state):\n",
    "        # Here, the action is random, but you can change this\n",
    "        input_tensor = torch.tensor(state.reshape(1,2).astype(np.float32))\n",
    "        q_values = self.dqn.q_network.forward(input_tensor)\n",
    "        greedy_action = int(q_values.argmax(1).numpy())\n",
    "\n",
    "        actions = np.arange(0,4)\n",
    "        probas = np.ones(4)*(self.epsilon/4)\n",
    "        probas[greedy_action] = 1-self.epsilon + (self.epsilon/4)\n",
    "        discrete_action = np.random.choice(actions,p=probas)\n",
    "\n",
    "        self.epsilon -= 0.001\n",
    "\n",
    "        action = self._discrete_action_to_continuous(discrete_action)\n",
    "        # Update the number of steps which the agent has taken\n",
    "        self.num_steps_taken += 1\n",
    "        # Store the state; this will be used later, when storing the transition\n",
    "        self.state = state\n",
    "        # Store the action; this will be used later, when storing the transition\n",
    "        self.action = action\n",
    "        return action\n",
    "\n",
    "    # Function to set the next state and distance, which resulted from applying action self.action at state self.state\n",
    "    def set_next_state_and_distance(self, next_state, distance_to_goal):\n",
    "        # Convert the distance to a reward\n",
    "        reward = 1 - distance_to_goal\n",
    "        # Create a transition\n",
    "        transition = (self.state, self.action, reward, next_state)\n",
    "        # Add this transition to the replay buffer\n",
    "        transition_discrete = (self.state, self._continuous_action_to_discrete(self.action), reward, next_state)\n",
    "        self.replay_buffer.append(transition_discrete)\n",
    "\n",
    "        if self.replay_buffer.is_full_enough():\n",
    "            mini_batch = self.replay_buffer.get_minibatch()\n",
    "            loss = self.dqn.train_q_network(mini_batch)\n",
    "\n",
    "        self.total_reward+= reward\n",
    "\n",
    "\n",
    "    # Function to get the greedy action for a particular state\n",
    "    def get_greedy_action(self,state):\n",
    "        input_tensor = torch.tensor(state.reshape(1,2).astype(np.float32))\n",
    "        q_values = self.dqn.q_network.forward(input_tensor)\n",
    "        action = self._discrete_action_to_continuous(int(q_values.argmax(1).numpy()))\n",
    "        return action\n",
    "\n",
    "        # Function to convert discrete action (as used by a DQN) to a continuous action (as used by the environment).\n",
    "    def _discrete_action_to_continuous(self, discrete_action):\n",
    "        if discrete_action == 0:  # Move right\n",
    "            continuous_action = np.array([0.02, 0], dtype=np.float32)\n",
    "        elif discrete_action == 1:#Move down\n",
    "            continuous_action = np.array([0, -0.02], dtype=np.float32)\n",
    "        elif discrete_action == 2:#Move left\n",
    "            continuous_action = np.array([-0.02, 0], dtype=np.float32)\n",
    "        else :#Move up\n",
    "            continuous_action = np.array([0, 0.02], dtype=np.float32)\n",
    "        return continuous_action\n",
    "\n",
    "    def _continuous_action_to_discrete(self, continuous_action):\n",
    "        if (continuous_action == np.array([0.02, 0], dtype=np.float32)).all():  # Move right\n",
    "            discrete_action = 0\n",
    "        elif (continuous_action == np.array([0, -0.02], dtype=np.float32)).all():#Move down\n",
    "            discrete_action = 1\n",
    "        elif (continuous_action == np.array([-0.02, 0], dtype=np.float32)).all():#Move left\n",
    "            discrete_action = 2\n",
    "        elif (continuous_action == np.array([0, 0.02], dtype=np.float32)).all() :#Move up\n",
    "            discrete_action = 3\n",
    "        else:\n",
    "            raise ValueError('not one of actions permited')\n",
    "        return discrete_action\n",
    "\n",
    "class DQN:\n",
    "\n",
    "    # The class initialisation function.\n",
    "    def __init__(self):\n",
    "        # Create a Q-network, which predicts the q-value for a particular state.\n",
    "        self.q_network = Network(input_dimension=2, output_dimension=4)\n",
    "        # Define the optimiser which is used when updating the Q-network. The learning rate determines how big each gradient step is during backpropagation.\n",
    "        self.optimiser = torch.optim.Adam(self.q_network.parameters(), lr=0.001)\n",
    "        self.target_network = Network(input_dimension=2, output_dimension=4)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    # Function that is called whenever we want to train the Q-network. Each call to this function takes in a transition tuple containing the data we use to update the Q-network.\n",
    "    def train_q_network(self, transition):\n",
    "        # Set all the gradients stored in the optimiser to zero.\n",
    "        self.optimiser.zero_grad()\n",
    "        # Calculate the loss for this transition.\n",
    "        loss = self._calculate_loss(transition)\n",
    "        # Compute the gradients based on this loss, i.e. the gradients of the loss with respect to the Q-network parameters.\n",
    "        loss.backward()\n",
    "        # Take one gradient step to update the Q-network.\n",
    "        self.optimiser.step()\n",
    "        # Return the loss as a scalar\n",
    "        loss_value = loss.item()\n",
    "        return loss_value\n",
    "\n",
    "\n",
    "    def update_target_network(self):\n",
    "        parameters_q = torch.nn.Module.state_dict(self.q_network)\n",
    "        torch.nn.Module.load_state_dict(self.target_network,parameters_q)\n",
    "\n",
    "    # Function to calculate the loss for a particular transition.\n",
    "    def _calculate_loss(self,transition):\n",
    "\n",
    "        gamma = 0.9\n",
    "        batch_size = len(transition)\n",
    "        transition = np.array(transition).reshape(batch_size,4)\n",
    "\n",
    "        states_batch = np.array([state.reshape(1,2) for state in transition[:,0]])\n",
    "        next_states_batch = np.array([state.reshape(1,2) for state in transition[:,3]])\n",
    "\n",
    "        ### extracting batches array from transition\n",
    "\n",
    "        input_batch =  states_batch.reshape(batch_size,2).astype(np.float32)\n",
    "        action_batch = transition[:,1].reshape(batch_size,1).astype(np.long)\n",
    "        reward_batch = transition[:,2].reshape(batch_size,1).astype(np.float32)\n",
    "        next_states_batch = next_states_batch.reshape(batch_size,2).astype(np.float32)\n",
    "\n",
    "        ### transforming arrays into tensors\n",
    "\n",
    "        input_tensor = torch.tensor(input_batch)\n",
    "        action_tensor = torch.tensor(action_batch)\n",
    "        reward_tensor = torch.tensor(reward_batch)\n",
    "        next_states_tensor = torch.tensor(next_states_batch)\n",
    "\n",
    "        ### Computing the predicted Q value for state s\n",
    "\n",
    "        q_values = self.q_network.forward(input_tensor)\n",
    "        q_values = torch.gather(q_values,1,action_tensor)\n",
    "\n",
    "        ### Computing the actual Q value for this step\n",
    "\n",
    "        #target_q_values = self.q_network.forward(next_states_tensor).detach()\n",
    "        target_q_values = self.target_network.forward(next_states_tensor)\n",
    "        target_q_values = torch.gather(target_q_values,1, target_q_values.argmax(1).view(batch_size,1))\n",
    "\n",
    "        predicted_sum_future_rewards = reward_tensor.add(target_q_values*gamma)\n",
    "        loss = torch.nn.MSELoss()(q_values, predicted_sum_future_rewards)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_greedy_action(self,state,mode):\n",
    "        best_reward = -100\n",
    "        best_action = -100\n",
    "\n",
    "        if mode==0:\n",
    "            for i in range(4):\n",
    "                input_state_action = np.append(state,i).reshape(1,3).astype(np.float32)\n",
    "                input_tensor = torch.tensor(input_state_action)\n",
    "                predicted_reward = self.q_network.forward(input_tensor)\n",
    "\n",
    "                if predicted_reward>best_reward:\n",
    "                    best_reward = predicted_reward\n",
    "                    best_action = i\n",
    "\n",
    "\n",
    "        elif mode==1:\n",
    "            input_tensor = torch.tensor(state.reshape(1,2).astype(np.float32))\n",
    "            q_values = self.q_network.forward(input_tensor)\n",
    "            best_action = int(q_values.argmax(1).numpy())\n",
    "\n",
    "        else:\n",
    "            raise ValueError('mode must be either 0 or 1')\n",
    "\n",
    "        return best_action\n",
    "\n",
    "    def get_qvalues(self,mode):\n",
    "\n",
    "        if mode == 0:\n",
    "            qvalues = np.zeros((10,10,4))\n",
    "            for i in range(10):\n",
    "                for j in range(10):\n",
    "                    for action in range(4):\n",
    "                        input_state_action = np.array([(0.05 + i*0.1),(0.05 + j*0.1),action]).reshape(1,3).astype(np.float32)\n",
    "                        input_tensor = torch.tensor(input_state_action)\n",
    "                        predicted_qvalue = self.q_network.forward(input_tensor)\n",
    "                        qvalues[i,j,action] = predicted_qvalue\n",
    "            return qvalues\n",
    "\n",
    "        elif mode ==1 :\n",
    "            qvalues = np.zeros((10,10,4))\n",
    "            for i in range(10):\n",
    "                for j in range(10):\n",
    "                    input_state = np.array([(0.05 + i*0.1),(0.05 + j*0.1)]).reshape(1,2).astype(np.float32)\n",
    "                    input_tensor = torch.tensor(input_state)\n",
    "                    predicted_qvalues = self.q_network.forward(input_tensor)\n",
    "                    qvalues[i,j,:] = predicted_qvalues.detach().numpy().reshape(4)\n",
    "            return qvalues\n",
    "\n",
    "        else:\n",
    "            raise ValueError('mode must be either 0 or 1')\n",
    "\n",
    "# The Network class inherits the torch.nn.Module class, which represents a neural network.\n",
    "class Network(torch.nn.Module):\n",
    "\n",
    "    # The class initialisation function. This takes as arguments the dimension of the network's input (i.e. the dimension of the state), and the dimension of the network's output (i.e. the dimension of the action).\n",
    "    def __init__(self, input_dimension, output_dimension):\n",
    "        # Call the initialisation function of the parent class.\n",
    "        super(Network, self).__init__()\n",
    "        # Define the network layers. This example network has two hidden layers, each with 100 units.\n",
    "        self.layer_1 = torch.nn.Linear(in_features=input_dimension, out_features=100)\n",
    "        self.layer_2 = torch.nn.Linear(in_features=100, out_features=100)\n",
    "        self.output_layer = torch.nn.Linear(in_features=100, out_features=output_dimension)\n",
    "\n",
    "    # Function which sends some input data through the network and returns the network's output. In this example, a ReLU activation function is used for both hidden layers, but the output layer has no activation function (it is just a linear layer).\n",
    "    def forward(self, input):\n",
    "        layer_1_output = torch.nn.functional.relu(self.layer_1(input))\n",
    "        layer_2_output = torch.nn.functional.relu(self.layer_2(layer_1_output))\n",
    "        output = self.output_layer(layer_2_output)\n",
    "        return output\n",
    "\n",
    "class ReplayBuffer(deque):\n",
    "    def __init__(self,min_size):\n",
    "        super().__init__([],10**6)\n",
    "        self.min_size = min_size\n",
    "\n",
    "    def is_full_enough(self):\n",
    "        return len(self)>=self.min_size\n",
    "\n",
    "    def get_minibatch(self):\n",
    "        idx = np.random.choice(np.arange(0,len(self)),self.min_size,replace=False)\n",
    "        try:\n",
    "            minibatch = []\n",
    "            for i in range(len(idx)):\n",
    "                minibatch += [self[idx[i]]]\n",
    "        except:\n",
    "            raise ValueError(\"Replay Buffer is not full enough\")\n",
    "\n",
    "        return np.array(minibatch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_greedy_policy_test(env,network):\n",
    "    test_agent = Agent(env)\n",
    "    test_agent.reset()\n",
    "    for step_num in range(20):\n",
    "        best_action = network.get_greedy_action(test_agent.state,1)\n",
    "        transition = test_agent.step('egreedy',epsilon = 0, action=best_action)\n",
    "    return test_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CID = 799240\n",
    "environment = Environment(display=False, magnification=500)\n",
    "\n",
    "goal_state = np.array([0.75, 0.85], dtype=np.float32)\n",
    "deltas = [0,0.005,0.01,0.1,0.5,1]\n",
    "rewards = []\n",
    "\n",
    "for i,delta_i in enumerate(deltas):\n",
    "    agent = Agent(environment)\n",
    "    dqn = DQN()\n",
    "    np.random.seed(CID)\n",
    "    torch.manual_seed(CID)\n",
    "    distance_i = []\n",
    "    replay_buffer = ReplayBuffer(min_size=50)\n",
    "    n_steps = 0\n",
    "    eps = 1\n",
    "    \n",
    "    while n_steps<500:\n",
    "        # Reset the environment for the start of the episode.\n",
    "        agent.reset()\n",
    "        # Loop over steps within this episode. The episode length here is 20.\n",
    "        for step_num in range(20):\n",
    "            # Step the agent once, and get the transition tuple for this step\n",
    "            n_steps+=1\n",
    "            if not replay_buffer.is_full_enough():\n",
    "                transition = agent.step('random')\n",
    "                replay_buffer.append(transition)\n",
    "            else:\n",
    "                best_action = dqn.get_greedy_action(agent.state,mode=1)\n",
    "                transition = agent.step('egreedy',epsilon = eps, action=best_action)\n",
    "                replay_buffer.append(transition)\n",
    "                mini_batch = replay_buffer.get_minibatch()\n",
    "                loss = dqn.train_q_network(mini_batch)\n",
    "                \n",
    "                if (eps-delta_i) >=0:\n",
    "                    eps -= delta_i\n",
    "                else:\n",
    "                    eps = 0\n",
    "                \n",
    "        dqn.update_target_network()\n",
    "        \n",
    "    test_agent = run_greedy_policy_test(environment,dqn)\n",
    "    rewards += [test_agent.total_reward]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(deltas,rewards)\n",
    "plt.xlabel('Delta')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Value of the loss function at each step of training with target network')\n",
    "plt.savefig('figs/fig4a.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonus_close(x):\n",
    "    if (1-x)>=0.1:\n",
    "        return 1-x\n",
    "    else:\n",
    "        return 1-x + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CID = 799240\n",
    "delta = 0.01\n",
    "goal_state = np.array([0.75, 0.85], dtype=np.float32)\n",
    "\n",
    "rf1 = lambda x:1-x\n",
    "rf2 = lambda x:(1-x)**2\n",
    "rf3 = lambda x:(1-x)**7\n",
    "rf4 = lambda x:x-1\n",
    "\n",
    "rf = [rf1,weird_distance]\n",
    "model_names = {0:}\n",
    "\n",
    "distance = []\n",
    "\n",
    "for i,reward_func_i in enumerate(rf):\n",
    "    environment = Environment(display=False, magnification=500)\n",
    "    agent = Agent(environment)\n",
    "    dqn = DQN()\n",
    "    distance_i = []\n",
    "    np.random.seed(CID)\n",
    "    torch.manual_seed(CID)\n",
    "    eps = 1\n",
    "    replay_buffer = ReplayBuffer(min_size=50)\n",
    "    n_steps = 0\n",
    "    \n",
    "    while n_steps<500:\n",
    "        # Reset the environment for the start of the episode.\n",
    "        agent.reset()\n",
    "        # Loop over steps within this episode. The episode length here is 20.\n",
    "        for step_num in range(20):\n",
    "            # Step the agent once, and get the transition tuple for this step\n",
    "            n_steps+=1\n",
    "            if not replay_buffer.is_full_enough():\n",
    "                transition = agent.step('random')\n",
    "                replay_buffer.append(transition)\n",
    "            else:\n",
    "                best_action = dqn.get_greedy_action(agent.state,mode=1)\n",
    "                transition = agent.step('egreedy',reward_func=reward_func_i,epsilon = eps, action=best_action)\n",
    "                replay_buffer.append(transition)\n",
    "                mini_batch = replay_buffer.get_minibatch()\n",
    "                loss = dqn.train_q_network(mini_batch)\n",
    "                \n",
    "                #test greedy only\n",
    "                last_state = run_greedy_policy_test(environment,dqn).state\n",
    "                last_distance_to_goal = np.linalg.norm(goal_state - last_state)\n",
    "                distance_i += [last_distance_to_goal]\n",
    "                \n",
    "                if (eps-delta) >=0:\n",
    "                    eps -= delta\n",
    "                else:\n",
    "                    eps = 0\n",
    "                \n",
    "        dqn.update_target_network()\n",
    "    distance += [np.array(distance_i)]\n",
    "    \n",
    "arr_distance = distance[0]\n",
    "\n",
    "for i in range(len(distance)-1):\n",
    "    arr_distance = np.vstack((arr_distance,distance[i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,dist in enumerate(arr_distance):\n",
    "    plt.plot(np.arange(50,n_steps),dist,label='{}-th distance'.format(i))\n",
    "    plt.legend()\n",
    "    plt.xticks(np.arange(50,501,50))\n",
    "    plt.xlabel('Number of steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Value of the loss function at each step of training with target network')\n",
    "    plt.savefig('figs/fig4b.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test greedy policy\n",
    "\n",
    "test_env = environment = Environment(display=True, magnification=500)\n",
    "agent = Agent(test_env)\n",
    "agent.reset()\n",
    "greedy_path = np.array([agent.state])\n",
    "time.sleep(1)\n",
    "for step_num in range(20):\n",
    "    best_action = dqn.get_greedy_action(agent.state,1)\n",
    "    transition = agent.step('egreedy',epsilon = 0, action=best_action)\n",
    "    greedy_path = np.append(greedy_path,[agent.state],axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = QNetworkViz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_q_values(dqn.get_qvalues(mode=1))\n",
    "viz.plot_grid()\n",
    "viz.plot_path(greedy_path)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array([0.1, 0], dtype=np.float32) == np.array([0.1, 0], dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([True ,False]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
